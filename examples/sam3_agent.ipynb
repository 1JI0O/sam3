{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SAM 3 Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook shows an example of how an MLLM can use SAM 3 as a tool, i.e., \"SAM 3 Agent\", to segment more complex text queries such as \"the leftmost child wearing blue vest\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Env Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First install `sam3` in your environment using the [installation instructions](https://github.com/facebookresearch/sam3?tab=readme-ov-file#installation) in the repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "# torch.cuda.set_device(4)\n",
        "# turn on tfloat32 for Ampere GPUs\n",
        "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# use bfloat16 for the entire notebook. If your card doesn't support it, try float16 instead\n",
        "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
        "\n",
        "# inference mode for the whole notebook. Disable if you need gradients\n",
        "torch.inference_mode().__enter__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jan 31 03:58:56 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-PCIE-40GB          Off |   00000000:1A:00.0 Off |                    0 |\n",
            "| N/A   42C    P0             38W /  250W |   24557MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   1  NVIDIA A100-PCIE-40GB          Off |   00000000:1B:00.0 Off |                    0 |\n",
            "| N/A   49C    P0             43W /  250W |   27861MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   2  NVIDIA A100-PCIE-40GB          Off |   00000000:3D:00.0 Off |                    0 |\n",
            "| N/A   40C    P0             37W /  250W |   23055MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   3  NVIDIA A100-PCIE-40GB          Off |   00000000:3E:00.0 Off |                    0 |\n",
            "| N/A   46C    P0             40W /  250W |   27901MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   4  NVIDIA A100-PCIE-40GB          Off |   00000000:88:00.0 Off |                    0 |\n",
            "| N/A   40C    P0             34W /  250W |       4MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   5  NVIDIA A100-PCIE-40GB          Off |   00000000:89:00.0 Off |                    0 |\n",
            "| N/A   36C    P0             39W /  250W |       4MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   6  NVIDIA A100-PCIE-40GB          Off |   00000000:B1:00.0 Off |                    0 |\n",
            "| N/A   36C    P0             37W /  250W |       4MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "|   7  NVIDIA A100-PCIE-40GB          Off |   00000000:B2:00.0 Off |                    0 |\n",
            "| N/A   35C    P0             38W /  250W |   22021MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A          843735      C   ...onda3/envs/rise/bin/python3.8      24548MiB |\n",
            "|    1   N/A  N/A          843736      C   ...onda3/envs/rise/bin/python3.8      27852MiB |\n",
            "|    2   N/A  N/A          922351      C   ...onda3/envs/rise/bin/python3.8      23046MiB |\n",
            "|    3   N/A  N/A          922352      C   ...onda3/envs/rise/bin/python3.8      27892MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "SAM3_ROOT = os.path.dirname(os.getcwd())\n",
        "os.chdir(SAM3_ROOT)\n",
        "\n",
        "# setup GPU to use -  A single GPU is good with the purpose of this demo\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
        "_ = os.system(\"nvidia-smi\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build SAM3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/haoxiang/sam3/sam3/model_builder.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import sam3\n",
        "from sam3 import build_sam3_image_model\n",
        "from sam3.model.sam3_image_processor import Sam3Processor\n",
        "\n",
        "sam3_root = os.path.dirname(sam3.__file__)\n",
        "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
        "# model = build_sam3_image_model(bpe_path=bpe_path)\n",
        "model = build_sam3_image_model(\n",
        "    bpe_path=f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\",\n",
        "    checkpoint_path=\"/data/haoxiang/sam3/models/facebook/sam3/sam3.pt\",  # 指向本地权重\n",
        "    load_from_HF=False  # 关键：禁止从 HF 下载\n",
        ")\n",
        "processor = Sam3Processor(model, confidence_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM Setup\n",
        "\n",
        "Config which MLLM to use, it can either be a model served by vLLM that you launch from your own machine or a model is served via external API. If you want to using a vLLM model, we also provided insturctions below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LLM_CONFIGS = {\n",
        "    # vLLM-served models\n",
        "    \"qwen3_vl_8b_thinking\": {\n",
        "        \"provider\": \"vllm\",\n",
        "        \"model\": \"Qwen/Qwen3-VL-8B-Thinking\",\n",
        "    },\n",
        "    # models served via external APIs\n",
        "    # add your own\n",
        "}\n",
        "\n",
        "model = \"qwen3_vl_8b_thinking\"\n",
        "LLM_API_KEY = \"DUMMY_API_KEY\"\n",
        "\n",
        "llm_config = LLM_CONFIGS[model]\n",
        "llm_config[\"api_key\"] = LLM_API_KEY\n",
        "llm_config[\"name\"] = model\n",
        "\n",
        "# setup API endpoint\n",
        "if llm_config[\"provider\"] == \"vllm\":\n",
        "    LLM_SERVER_URL = \"http://0.0.0.0:8001/v1\"  # replace this with your vLLM server address as needed\n",
        "else:\n",
        "    LLM_SERVER_URL = llm_config[\"base_url\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup vLLM server \n",
        "This step is only required if you are using a model served by vLLM, skip this step if you are calling LLM using an API like Gemini and GPT.\n",
        "\n",
        "* Install vLLM (in a separate conda env from SAM 3 to avoid dependency conflicts).\n",
        "  ```bash\n",
        "    conda create -n vllm python=3.12\n",
        "    pip install vllm --extra-index-url https://download.pytorch.org/whl/cu128\n",
        "  ```\n",
        "* Start vLLM server on the same machine of this notebook\n",
        "  ```bash\n",
        "    # qwen 3 VL 8B thinking\n",
        "    vllm serve Qwen/Qwen3-VL-8B-Thinking --tensor-parallel-size 4 --allowed-local-media-path / --enforce-eager --port 8001\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run SAM3 Agent Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from IPython.display import display, Image\n",
        "from sam3.agent.client_llm import send_generate_request as send_generate_request_orig\n",
        "from sam3.agent.client_sam3 import call_sam_service as call_sam_service_orig\n",
        "from sam3.agent.inference import run_single_image_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "output": {
          "id": 689664053567678,
          "loadingStatus": "loaded"
        }
      },
      "outputs": [],
      "source": [
        "# prepare input args and run single image inference\n",
        "image = \"assets/images/test_image.jpg\"\n",
        "prompt = \"the leftmost child wearing blue vest\"\n",
        "image = os.path.abspath(image)\n",
        "send_generate_request = partial(send_generate_request_orig, server_url=LLM_SERVER_URL, model=llm_config[\"model\"], api_key=llm_config[\"api_key\"])\n",
        "call_sam_service = partial(call_sam_service_orig, sam3_processor=processor)\n",
        "output_image_path = run_single_image_inference(\n",
        "    image, prompt, llm_config, send_generate_request, call_sam_service,\n",
        "    debug=True, output_dir=\"agent_output\"\n",
        ")\n",
        "\n",
        "# display output\n",
        "if output_image_path is not None:\n",
        "    display(Image(filename=output_image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "be59e249-6c09-4634-a9e7-1f06fd233c42",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "sam3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
