{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "import sam3\n",
    "import torch\n",
    "import glob\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")\n",
    "# video_path = f\"{sam3_root}/assets/videos/camera0_rgb.mp4\"\n",
    "# video_path = \"/data/haoxiang/acp/flip_v3/scene_0001/cam_104122060902/color\"\n",
    "# video_path = \"/data/haoxiang/data/FLIPPING_v3/train/scene_0001/cam_104122060902/color\"\n",
    "# video_path = \"haoxiang/realdata_rise2_ready/train/task_0014_user_0020_scene_0001_cfg_0001/cam_104122063550/color\"\n",
    "video_path = '/data/haoxiang/data/airexo2/task_0013/train/scene_0001/cam_105422061350/color'\n",
    "\n",
    "# use all available GPUs on the machine\n",
    "gpus_to_use = range(torch.cuda.device_count())\n",
    "\n",
    "from sam3.model_builder import build_sam3_video_predictor\n",
    "\n",
    "# Notebook 反复执行时，先清理上一次 predictor 与默认进程组，避免 NCCL 初始化断言失败\n",
    "if \"predictor\" in globals() and predictor is not None:\n",
    "    try:\n",
    "        predictor.shutdown()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] predictor.shutdown() failed: {e}\")\n",
    "    finally:\n",
    "        predictor = None\n",
    "\n",
    "if torch.distributed.is_available() and torch.distributed.is_initialized():\n",
    "    try:\n",
    "        torch.distributed.destroy_process_group()\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] destroy_process_group() failed: {e}\")\n",
    "\n",
    "predictor = build_sam3_video_predictor(\n",
    "    checkpoint_path=\"/data/haoxiang/sam3/models/facebook/sam3/sam3.pt\",\n",
    "    gpus_to_use=gpus_to_use,\n",
    "    # 关闭 temporal disambiguation，避免 hotstart 在早期过滤掉候选 obj_id\n",
    "    apply_temporal_disambiguation=False,\n",
    ")\n",
    "\n",
    "from sam3.visualization_utils import (\n",
    "    load_frame,\n",
    "    prepare_masks_for_visualization,\n",
    "    visualize_formatted_frame_output,\n",
    ")\n",
    "\n",
    "# font size for axes titles\n",
    "plt.rcParams[\"axes.titlesize\"] = 12\n",
    "plt.rcParams[\"figure.titlesize\"] = 12\n",
    "\n",
    "# 工具函数\n",
    "\n",
    "def propagate_in_video(predictor, session_id, propagation_direction=\"forward\"):\n",
    "    # 单向传播，避免默认 both 时同一帧在不同方向被覆盖\n",
    "    outputs_per_frame = {}\n",
    "    for response in predictor.handle_stream_request(\n",
    "        request=dict(\n",
    "            type=\"propagate_in_video\",\n",
    "            session_id=session_id,\n",
    "            propagation_direction=propagation_direction,\n",
    "        )\n",
    "    ):\n",
    "        outputs_per_frame[response[\"frame_index\"]] = response[\"outputs\"]\n",
    "\n",
    "    return outputs_per_frame\n",
    "\n",
    "\n",
    "def abs_to_rel_coords(coords, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"):\n",
    "    \"\"\"Convert absolute coordinates to relative coordinates (0-1 range)\n",
    "\n",
    "    Args:\n",
    "        coords: List of coordinates\n",
    "        coord_type: 'point' for [x, y] or 'box' for [x, y, w, h]\n",
    "    \"\"\"\n",
    "    if coord_type == \"point\":\n",
    "        return [[x / IMG_WIDTH, y / IMG_HEIGHT] for x, y in coords]\n",
    "    elif coord_type == \"box\":\n",
    "        return [\n",
    "            [x / IMG_WIDTH, y / IMG_HEIGHT, w / IMG_WIDTH, h / IMG_HEIGHT]\n",
    "            for x, y, w, h in coords\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown coord_type: {coord_type}\")\n",
    "\n",
    "# load \"video_frames_for_vis\" for visualization purposes (they are not used by the model)\n",
    "if isinstance(video_path, str) and video_path.endswith(\".mp4\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    video_frames_for_vis = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        video_frames_for_vis.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "    cap.release()\n",
    "else:\n",
    "    video_frames_for_vis = glob.glob(os.path.join(video_path, \"*.png\"))\n",
    "    try:\n",
    "        # integer sort instead of string sort (so that e.g. \"2.jpg\" is before \"11.jpg\")\n",
    "        video_frames_for_vis.sort(\n",
    "            key=lambda p: int(os.path.splitext(os.path.basename(p))[0])\n",
    "        )\n",
    "    except ValueError:\n",
    "        # fallback to lexicographic sort if the format is not \"<frame_index>.jpg\"\n",
    "        print(\n",
    "            f'frame names are not in \"<frame_index>.png\" format: {video_frames_for_vis[:5]=}, '\n",
    "            f\"falling back to lexicographic sort.\"\n",
    "        )\n",
    "        video_frames_for_vis.sort()\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"start_session\",\n",
    "        resource_path=video_path,\n",
    "    )\n",
    ")\n",
    "session_id = response[\"session_id\"]\n",
    "\n",
    "# # note: in case you already ran one text prompt and now want to switch to another text prompt\n",
    "# # it's required to reset the session first (otherwise the results would be wrong)\n",
    "# _ = predictor.handle_request(\n",
    "#     request=dict(\n",
    "#         type=\"reset_session\",\n",
    "#         session_id=session_id,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# 开始使用prompt\n",
    "\n",
    "# prompt_text_str = \"robot and cable on the right side\" \n",
    "prompt_text_str = \"black claw\" \n",
    "\n",
    "# prompt_text_str = \"robot arm with black cable\"\n",
    "# prompt_text_str = \"black end-effector\"\n",
    "frame_idx = 0  # add a text prompt on frame 0\n",
    "\n",
    "# 这里idx需要从0开始，不然之前的不识别\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\", # 既然是add_prompt，这里其实是对这个predictor进行操作，request中内容会存储在class中\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        text=prompt_text_str,\n",
    "    )\n",
    ")\n",
    "out = response[\"outputs\"]\n",
    "\n",
    "# 先画一张出来测试一下\n",
    "\n",
    "plt.close(\"all\")\n",
    "visualize_formatted_frame_output(\n",
    "    frame_idx,\n",
    "    video_frames_for_vis,\n",
    "    outputs_list=[prepare_masks_for_visualization({frame_idx: out})],\n",
    "    titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "    figsize=(6, 4),\n",
    ")\n",
    "\n",
    "# ------------------------------- #\n",
    "\n",
    "# response = predictor.handle_request(\n",
    "#     request=dict(\n",
    "#         type=\"remove_object\",\n",
    "#         session_id=session_id,\n",
    "#         obj_id=1,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# now we propagate the outputs from frame 0 to the end of the video and collect all outputs\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id, propagation_direction=\"forward\")\n",
    "print(len(outputs_per_frame))\n",
    "\n",
    "# finally, we reformat the outputs for visualization and plot the outputs every 60 frames\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "# print(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )\n",
    "\n",
    "# sam3的逻辑是，要先跑一遍原始的视频处理，然后才能添加点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794948c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标注点\n",
    "\n",
    "# !pip install ipympl ipywidgets\n",
    "\n",
    "# 启用 widget 后端\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.widgets import Button\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets # 导入控制输出的工具\n",
    "from IPython.display import display\n",
    "\n",
    "# 1. 创建一个专门的输出区域\n",
    "out = widgets.Output()\n",
    "\n",
    "# 选择第几帧标注\n",
    "draw_frame_idx = 0\n",
    "\n",
    "# 获取数据并转换 (这里假设你的环境已经有 load_frame 等函数)\n",
    "frame_data = load_frame(video_frames_for_vis[draw_frame_idx])\n",
    "if hasattr(frame_data, 'cpu'): frame_data = frame_data.cpu().numpy()\n",
    "if frame_data.dtype == np.float32 or frame_data.dtype == np.float64:\n",
    "    if frame_data.max() <= 1.05: frame_data = frame_data * 255.0\n",
    "    frame_data = frame_data.astype(np.uint8)\n",
    "\n",
    "sample_img = Image.fromarray(frame_data)\n",
    "\n",
    "# 创建画布\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n",
    "plt.subplots_adjust(bottom=0.2) \n",
    "ax.imshow(sample_img)\n",
    "ax.set_title(\"Click to add points\")\n",
    "\n",
    "coords = []\n",
    "# 额外保存绘制对象，便于清空\n",
    "point_artists = []\n",
    "\n",
    "def onclick(event):\n",
    "    if event.inaxes == ax:\n",
    "        x, y = int(event.xdata), int(event.ydata)\n",
    "        coords.append([x, y])\n",
    "\n",
    "        # 注意：plot 返回列表，取第一个 artist\n",
    "        marker = ax.plot(x, y, 'r*', markersize=15)[0]\n",
    "        label = ax.text(\n",
    "            x + 10, y, f'({x},{y})', color='yellow', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='black', alpha=0.7)\n",
    "        )\n",
    "\n",
    "        point_artists.extend([marker, label])\n",
    "        fig.canvas.draw_idle()\n",
    "\n",
    "\n",
    "def export_points(event):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        for i, p in enumerate(coords):\n",
    "            suffix = \",\" if i < len(coords) - 1 else \"\"\n",
    "            print(f\"        [{p[0]}, {p[1]}]{suffix}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "\n",
    "\n",
    "def reset_points(event):\n",
    "    # 1) 清空坐标\n",
    "    coords.clear()\n",
    "\n",
    "    # 2) 从画布移除已画的点和文字\n",
    "    for artist in point_artists:\n",
    "        artist.remove()\n",
    "    point_artists.clear()\n",
    "\n",
    "    # 3) 清空输出区\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        print(\"已清空，重新开始标注\")\n",
    "\n",
    "    # 4) 刷新画布\n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "# 按钮1：导出\n",
    "ax_btn_export = plt.axes([0.30, 0.05, 0.2, 0.075])\n",
    "btn_export = Button(ax_btn_export, 'Export Points')\n",
    "btn_export.on_clicked(export_points)\n",
    "\n",
    "# 按钮2：重置\n",
    "ax_btn_reset = plt.axes([0.55, 0.05, 0.2, 0.075])\n",
    "btn_reset = Button(ax_btn_reset, 'Reset Points')\n",
    "btn_reset.on_clicked(reset_points)\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2. 在最后显示这个输出区域\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107987f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于标注，添加 positive 和 negative 点\n",
    "# 左边的机械臂是id_1 右边的机械臂是id_0\n",
    "\n",
    "IMG_WIDTH, IMG_HEIGHT = sample_img.size\n",
    "\n",
    "# frame_idx = 120 # 要和标注帧匹配！\n",
    "# frame_idx = draw_frame_idx # 直接用上面的变量\n",
    "\n",
    "\n",
    "# # ------------------------------- #\n",
    "# # 处理 id_1 (左边)\n",
    "# # ------------------------------- #\n",
    "\n",
    "# frame_idx = 0\n",
    "\n",
    "# obj_id = 1\n",
    "# points_abs = np.array(\n",
    "#     [\n",
    "#         [261, 369]\n",
    "#     ]\n",
    "# )\n",
    "# # positive clicks have label 1, while negative clicks have label 0\n",
    "# # labels = np.array([1, 0, 0, 1])\n",
    "# labels = np.array([0])\n",
    "\n",
    "# # convert points and labels to tensors; also convert to relative coordinates\n",
    "# points_tensor = torch.tensor(\n",
    "#     abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"),\n",
    "#     dtype=torch.float32,\n",
    "# )\n",
    "# points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "# response = predictor.handle_request(\n",
    "#     request=dict(\n",
    "#         type=\"add_prompt\",\n",
    "#         session_id=session_id,\n",
    "#         frame_index=frame_idx,\n",
    "#         points=points_tensor,\n",
    "#         point_labels=points_labels_tensor,\n",
    "#         obj_id=obj_id,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# ------------------------------- #\n",
    "# 处理 id_0 (右边)\n",
    "# ------------------------------- #\n",
    "\n",
    "# frame_idx = 120\n",
    "\n",
    "# obj_id = 0\n",
    "# points_abs = np.array(\n",
    "#     [\n",
    "#         [973, 356]\n",
    "#     ]\n",
    "# )\n",
    "# # positive clicks have label 1, while negative clicks have label 0\n",
    "# # labels = np.array([1, 0, 0, 1])\n",
    "# labels = np.array([0])\n",
    "\n",
    "# # convert points and labels to tensors; also convert to relative coordinates\n",
    "# points_tensor = torch.tensor(\n",
    "#     abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"),\n",
    "#     dtype=torch.float32,\n",
    "# )\n",
    "# points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "# response = predictor.handle_request(\n",
    "#     request=dict(\n",
    "#         type=\"add_prompt\",\n",
    "#         session_id=session_id,\n",
    "#         frame_index=frame_idx,\n",
    "#         points=points_tensor,\n",
    "#         point_labels=points_labels_tensor,\n",
    "#         obj_id=obj_id,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# ------------------------------- #\n",
    "# 处理 id_0 (右边)\n",
    "# ------------------------------- #\n",
    "\n",
    "frame_idx = 120\n",
    "\n",
    "obj_id = 0\n",
    "points_abs = np.array(\n",
    "    [\n",
    "        [962, 350]\n",
    "    ]\n",
    ")\n",
    "# positive clicks have label 1, while negative clicks have label 0\n",
    "# labels = np.array([1, 0, 0, 1])\n",
    "labels = np.array([0])\n",
    "\n",
    "# convert points and labels to tensors; also convert to relative coordinates\n",
    "points_tensor = torch.tensor(\n",
    "    abs_to_rel_coords(points_abs, IMG_WIDTH, IMG_HEIGHT, coord_type=\"point\"),\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "points_labels_tensor = torch.tensor(labels, dtype=torch.int32)\n",
    "\n",
    "response = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"add_prompt\",\n",
    "        session_id=session_id,\n",
    "        frame_index=frame_idx,\n",
    "        points=points_tensor,\n",
    "        point_labels=points_labels_tensor,\n",
    "        obj_id=obj_id,\n",
    "    )\n",
    ")\n",
    "\n",
    "# ------------------------------- #\n",
    "# 可视化\n",
    "# ------------------------------- #\n",
    "\n",
    "# response = predictor.handle_request(\n",
    "#     request=dict(\n",
    "#         type=\"remove_object\",\n",
    "#         session_id=session_id,\n",
    "#         obj_id=1,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# now we propagate the outputs from frame 0 to the end of the video and collect all outputs\n",
    "outputs_per_frame = propagate_in_video(predictor, session_id, propagation_direction=\"forward\")\n",
    "print(len(outputs_per_frame))\n",
    "\n",
    "# finally, we reformat the outputs for visualization and plot the outputs every 60 frames\n",
    "outputs_per_frame = prepare_masks_for_visualization(outputs_per_frame)\n",
    "# print(outputs_per_frame)\n",
    "\n",
    "vis_frame_stride = 60\n",
    "plt.close(\"all\")\n",
    "for frame_idx in range(0, len(outputs_per_frame), vis_frame_stride):\n",
    "    visualize_formatted_frame_output(\n",
    "        frame_idx,\n",
    "        video_frames_for_vis,\n",
    "        outputs_list=[outputs_per_frame],\n",
    "        titles=[\"SAM 3 Dense Tracking outputs\"],\n",
    "        figsize=(6, 4),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290aa281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为 ProPainter 保存 mask 序列，并进行膨胀处理\n",
    "\n",
    "def save_masks_for_propainter(outputs_per_frame, video_frames, output_dir=\"propainter_masks\", \n",
    "                              dilate_radius=8):  # 新增参数：膨胀半径\n",
    "    \"\"\"\n",
    "    为 ProPainter 生成 mask 序列，并进行膨胀处理。\n",
    "    \n",
    "    Args:\n",
    "        outputs_per_frame: {frame_idx: {obj_id: binary_mask}} \n",
    "        video_frames: 视频帧路径列表或已加载的帧数组列表\n",
    "        output_dir: 输出目录\n",
    "        dilate_radius: 膨胀半径（像素）。值越大，Mask 扩得越粗。Mirage/ProPainter 一般建议 5-15 左右。\n",
    "                       如果为 0 则不膨胀。\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # --- 1. 获取尺寸 ---\n",
    "    first_frame = video_frames[0]\n",
    "    if isinstance(first_frame, str):\n",
    "        sample_img = Image.open(first_frame)\n",
    "        img_width, img_height = sample_img.size\n",
    "    else:\n",
    "        if isinstance(first_frame, torch.Tensor):\n",
    "            first_frame = first_frame.cpu().numpy()\n",
    "        if first_frame.ndim == 4:\n",
    "            first_frame = first_frame[0]\n",
    "        img_height, img_width = first_frame.shape[0], first_frame.shape[1]\n",
    "    \n",
    "    print(f\"视频尺寸: {img_width}x{img_height}, 共 {len(video_frames)} 帧\")\n",
    "    \n",
    "    # --- 2. 准备膨胀 Kernel (核心修改部分) ---\n",
    "    # Kernel 大小通常是 (2*radius + 1)，例如半径 5 对应 11x11 的核\n",
    "    kernel = None\n",
    "    if dilate_radius > 0:\n",
    "        kernel_size = 2 * dilate_radius + 1\n",
    "        # 创建矩形结构元素 (也可以用 cv2.getStructuringElement(cv2.MORPH_ELLIPSE, ...) 创建圆形的)\n",
    "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "        print(f\"启用 Mask 膨胀: 半径 {dilate_radius} (Kernel {kernel_size}x{kernel_size})\")\n",
    "\n",
    "    saved_paths = []\n",
    "    \n",
    "    for frame_idx in range(len(video_frames)):\n",
    "        \n",
    "        # --- 3. 生成基础 Mask ---\n",
    "        # 初始化全黑图\n",
    "        combined_mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "        \n",
    "        if frame_idx in outputs_per_frame and outputs_per_frame[frame_idx]:\n",
    "            obj_dict = outputs_per_frame[frame_idx]\n",
    "            \n",
    "            for obj_id, mask in obj_dict.items():\n",
    "                \n",
    "                if obj_id != 2:\n",
    "                    continue\n",
    "\n",
    "                if isinstance(mask, torch.Tensor):\n",
    "                    mask = mask.cpu().numpy()\n",
    "                if mask.ndim > 2:\n",
    "                    mask = mask.squeeze()\n",
    "                \n",
    "                # 合并 Mask\n",
    "                # mask > 0 会得到一个布尔矩阵 (True/False)。\n",
    "                # .astype(np.uint8) * 255 把 True 变成 255（白色），False 变成 0（黑色）。\n",
    "                binary_mask = (mask > 0).astype(np.uint8) * 255\n",
    "                combined_mask = np.maximum(combined_mask, binary_mask)\n",
    "        \n",
    "        # --- 4. 执行膨胀 ---\n",
    "        # 只有当 Mask 不是全黑时才膨胀\n",
    "        if dilate_radius > 0 and np.any(combined_mask):\n",
    "            # iterations=1 表示执行一次膨胀，配合 kernel 大小控制扩充程度\n",
    "            combined_mask = cv2.dilate(combined_mask, kernel, iterations=1)\n",
    "\n",
    "        # --- 5. 保存 ---\n",
    "        filename = f\"{frame_idx:05d}.png\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        Image.fromarray(combined_mask, mode='L').save(filepath)\n",
    "        saved_paths.append(filepath)\n",
    "        \n",
    "        if frame_idx % 20 == 0:\n",
    "            has_target = (frame_idx in outputs_per_frame and outputs_per_frame[frame_idx])\n",
    "            status = \"✓ 有目标\" if has_target else \"✗ 全黑\"\n",
    "            # print(f\"处理帧 {frame_idx}/{len(video_frames)}: {status}\")\n",
    "    \n",
    "    print(f\"\\n完成！已生成 {len(saved_paths)} 张 mask (膨胀半径={dilate_radius}) 至: {os.path.abspath(output_dir)}\")\n",
    "    return saved_paths\n",
    "\n",
    "# --- 调用示例 ---\n",
    "mask_paths = save_masks_for_propainter(\n",
    "    outputs_per_frame=outputs_per_frame,\n",
    "    video_frames=video_frames_for_vis,\n",
    "    # output_dir=\"/data/haoxiang/propainter/masks_FLIPPING_v3_scene0001\",\n",
    "    output_dir=\"/data/haoxiang/propainter/masks_FLIPPING_v3_scene0001_arm_only\",\n",
    "    dilate_radius=15  # <--- 这里控制膨胀程度，一般 8-15 比较合适\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c70aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, close the inference session to free its GPU resources\n",
    "# (you may start a new session on another video)\n",
    "_ = predictor.handle_request(\n",
    "    request=dict(\n",
    "        type=\"close_session\",\n",
    "        session_id=session_id,\n",
    "    )\n",
    ")\n",
    "\n",
    "# after all inference is done, we can shutdown the predictor\n",
    "# to free up the multi-GPU process group\n",
    "predictor.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
